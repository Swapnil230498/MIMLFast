{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c90fedc3",
   "metadata": {},
   "source": [
    "IMPORTING ALL THE NEEDED LIBRARIES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "865654f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c061c05c",
   "metadata": {},
   "source": [
    "LOADING THE DATA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9184c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'train_data', 'test_data', 'train_targets', 'test_targets'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=loadmat(r\"example_data.mat\")\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "137f5d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets=data[\"train_targets\"]\n",
    "train_data=data['train_data']\n",
    "test_data=data['test_data']\n",
    "test_targets=data['test_targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e88820b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f477b909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(243, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec6ef33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa7419c1",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e45fc27",
   "metadata": {},
   "source": [
    "CREATING A FUNCTION FOR TRAINING MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1ab3edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MIML_train(train_data, train_targets, W, V, costs, norm_up, step_size0, num_sub, AW, AV, Anum, trounds, lambd, opts):\n",
    "    average_begin = opts['average_begin']\n",
    "    average_size = opts['average_size']\n",
    "    n, n_class = train_targets.shape\n",
    "    # tmpnums: a vector consisting of no of labels that are relevant in each bags\n",
    "    tmpnums = np.sum(train_targets >= 1, axis=1)\n",
    "    # Initialize an empty array to hold the pairs\n",
    "    train_pairs = []\n",
    "\n",
    "    # Loop through each row of train_targets\n",
    "    for i in range(n):\n",
    "        # Get the indices of all elements in the row that are greater than or equal to 1\n",
    "        idx = np.where(train_targets[i,:] >= 1)[0]\n",
    "        # Loop through each index(present in idx) and add the (i, j) pair to train_pairs\n",
    "        for j in idx:\n",
    "            train_pairs.append((i, j))\n",
    "    # Convert train_pairs to a numpy array\n",
    "    train_pairs = np.array(train_pairs)\n",
    "    n_pairs = len(train_pairs)\n",
    "    random_idx = np.random.permutation(n_pairs) #generates random nos array in range n_pairs(1147)\n",
    "    trounds = int(trounds)\n",
    "    \n",
    "    for i in range(n_pairs):\n",
    "        '''We are storing the index value of random bags one by one which we obtain from the random_idx '''\n",
    "        idx_ins = int(train_pairs[random_idx[i]][0])\n",
    "#         print('idx_ins',idx_ins)\n",
    "        '''This is how we load the instances present in each bag:'''\n",
    "#         xbag = train_data[idx_ins]\n",
    "        xbag = train_data[:][idx_ins][0]\n",
    "        '''This is how we load the label as well for each random bags one by one since loop\n",
    "        will take onebyone'''\n",
    "        idx_class = int(train_pairs[random_idx[i]][1])\n",
    "        '''Now lets go for irrelavent labels:'''\n",
    "        '''if idx_class=dummy_variable(2),then idx_irr contains all the irrelevant rows only\n",
    "        otherwise it contains dummy_variable as well as all irrelevant variables'''\n",
    "        if idx_class == n_class:\n",
    "            idx_irr = np.where(train_targets[idx_ins, :] <= 0)[0]\n",
    "        else:\n",
    "            idx_irr = np.where(train_targets[idx_ins, :] != 1)[0]\n",
    "        n_irr = len(idx_irr)\n",
    "\n",
    "\n",
    "        Wy=W[:, idx_class*num_sub : (idx_class+1)*num_sub] \n",
    "#         print('shape of Wy',Wy.shape)\n",
    "#         print('shape of V', V.shape)\n",
    "\n",
    "        Vbag = V @ xbag\n",
    "#         print('shape of Vbag: ',Vbag.shape)\n",
    "        \n",
    "        \n",
    "        fs = np.dot(Wy.T, Vbag)\n",
    "        idx_max_class = np.argmax(fs, axis=0)\n",
    "        fs = np.max(fs, axis=0)\n",
    "\n",
    "        fy = np.max(fs)\n",
    "        \n",
    "# # #      idx_max_ins1 is a index of KEY INSTANCE for that label\n",
    "        idx_max_ins1 = np.argmax(fs)\n",
    "    \n",
    "\n",
    "        idx_max_class = idx_max_class[idx_max_ins1]\n",
    "#         print('idx_max_class: ',idx_max_class)\n",
    "        Wy = Wy[:, idx_max_class]\n",
    "#         print('shape of Wy updated: ',Wy.shape)\n",
    "\n",
    "#         if 1:  # two optional implementation, switch to 0 to use the matlab code\n",
    "#             print(\"its working\")\n",
    "# #             j, idx_pick, idx_max_pick, fyn, idx_max_ins2 = sample_max1_small(n_irr, idx_irr, W, Vbag, fy, num_sub, np.random.rand())\n",
    "#         else:\n",
    "        '''lets go to next step:'''\n",
    "        '''Doing same things like what we did earlier for the relevant labels. see copy for explaination.'''\n",
    "        for j in range(n_irr):\n",
    "            idx_pick = idx_irr[random.randint(0, n_irr-1)]\n",
    "            Wyn = W[:, (idx_pick)*num_sub : (idx_pick+1)*num_sub]\n",
    "            fs = np.dot(Wyn.T, Vbag)\n",
    "            idx_max_pick = np.argmax(fs, axis=0)\n",
    "#             print('idx_max_pick: ',idx_max_pick)\n",
    "            fs = np.max(fs, axis=0)\n",
    "#             print('fs: ',fs)\n",
    "            fyn = np.max(fs)\n",
    "            idx_max_ins2 = np.argmax(fyn)\n",
    "#             print(\"idx_max_ins2:\", idx_max_ins2)\n",
    "            idx_max_pick = idx_max_pick[idx_max_ins2]\n",
    "#             print(\"idx_max_pick updated: \",idx_max_pick)\n",
    "            if fyn > fy-1:\n",
    "                break\n",
    "\n",
    "        if fyn > fy-1:  # make a gradient step, N=j;\n",
    "            step_size = step_size0 / (1 + lambd * trounds * step_size0)\n",
    "#             print(\"step_size: \",step_size)\n",
    "            trounds += 1\n",
    "            Wyn = W[:, (idx_pick)*num_sub + idx_max_pick]\n",
    "#             print('shape of Wyn: ',Wyn.shape)\n",
    "            loss = costs[int(np.floor(n_irr/(j+1)))]\n",
    "\n",
    "       # ''' Now weights are updated using gradient descent and stored in tmp1'''\n",
    "            tmp1 = Wy + step_size * loss * Vbag[:, idx_max_ins1]\n",
    "    #         print('shape of tmp1: ',tmp1.shape)\n",
    "            if opts['norm'] != 0: #This line checks whether normalization of the weight matrix is enabled.\n",
    "                tmp3 = np.linalg.norm(tmp1) #This line calculates the L2 norm of the temporary variable tmp1.\n",
    "                if tmp3 > norm_up:\n",
    "                    tmp1 = tmp1 * norm_up / tmp3\n",
    "    #                 print('normalized tmp1 shape: ',tmp1.shape)\n",
    "            tmp2 = Wyn - step_size * loss * Vbag[:, idx_max_ins2]\n",
    "            if opts['norm'] != 0:\n",
    "                tmp3 = np.linalg.norm(tmp2)\n",
    "                if tmp3 > norm_up:\n",
    "                    tmp2 = tmp2 * norm_up / tmp3\n",
    "    #                 print('normalized tmp2 shape: ',tmp2.shape)\n",
    "\n",
    "\n",
    "                '''Similarly we now update the V matrix:'''\n",
    "    #         V = V - step_size*loss*(W[:,[(idx_pick)*num_sub+idx_max_pick, (idx_class)*num_sub+idx_max_class]] @ np.array([xbag[:,idx_max_ins2],-xbag[:,idx_max_ins1]]).T) #(Wyn*xins2'-Wy*xins1');\n",
    "    #         W_subset = np.hstack(((W[:, (idx_pick)*num_sub+idx_max_pick]),( W[:, (idx_class)*num_sub+idx_max_class])))\n",
    "            W_subset = np.hstack(((W[:, (idx_pick)*num_sub+idx_max_pick]), (W[:, (idx_class)*num_sub+idx_max_class]))).reshape(-1, 2)\n",
    "\n",
    "    #         print('w_subset shape: ', W_subset.shape)\n",
    "    #         xbag_subset = np.hstack((xbag[:, idx_max_ins2], -xbag[:, idx_max_ins1]))\n",
    "            xbag_subset = np.hstack((xbag[:, idx_max_ins2], -xbag[:, idx_max_ins1])).reshape(2, -1)\n",
    "\n",
    "    #         print('xbag_subset shape: ', xbag_subset.shape)\n",
    "            V = V - step_size * loss * np.dot(W_subset, xbag_subset)\n",
    "\n",
    "            '''Finally we update our weights:'''\n",
    "            W[:,(idx_class)*num_sub+idx_max_class] = tmp1\n",
    "            W[:,(idx_pick)*num_sub+idx_max_pick] = tmp2\n",
    "\n",
    "            if opts['norm'] != 0:\n",
    "    #             norms = np.sqrt(np.sum(V**2, axis=0))\n",
    "                norms = np.linalg.norm(V, axis=0)\n",
    "\n",
    "                idx_down = np.where(norms > norm_up)[0]\n",
    "    #             print('shape of idx_down: ', idx_down.shape)\n",
    "                if len(idx_down) > 0:\n",
    "#                     norms = norms[norms<=norm_up]\n",
    "                    norms = norms[norms > norm_up]\n",
    "\n",
    "    #                 print('shape of norms: ', norms.shape)\n",
    "                    for k in range(len(idx_down)):\n",
    "                        V[:, idx_down[k]] = V[:, idx_down[k]]*norm_up/norms[k]\n",
    "#         print(' ')\n",
    "# Now AW, AV and Anum are our average weights which we will be needing in order to predict our test data.\n",
    "# AW/Anum and AV/Anum are actually average and not AW and AV\n",
    "        if trounds > average_begin and i % average_size == 0:\n",
    "            AW = AW + W\n",
    "            AV = AV + V\n",
    "            Anum = Anum + 1\n",
    "                \n",
    "    return W,V,AW,AV,Anum,trounds          \n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94d3fd9",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7b0426",
   "metadata": {},
   "source": [
    "CREATING A FUNCTION FOR TESTING OUR MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fa5d62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MIML_test(test_data, W, V, num_sub):\n",
    "    # Set the number of top indices to select\n",
    "    firstk = 5\n",
    "    \n",
    "    # Get the number of bags\n",
    "    n = len(test_data)\n",
    "    \n",
    "    # Create an empty array for storing the max score for each bag and label combination\n",
    "    pres = np.full((n, int(W.shape[1]/num_sub)), -np.inf)\n",
    "\n",
    "    \n",
    "    # Calculate the matrix product of the transpose of V and W\n",
    "    WV = V.T @ W\n",
    "#     print('shape of WV: ',WV.shape)\n",
    "    # Loop over each bag in test_data and each sublabels\n",
    "    for i in range(n):\n",
    "#         xbag = test_data[i]\n",
    "        xbag = test_data[:][i][0]\n",
    "#         print('shape of xbag: ',xbag.shape)\n",
    "        for j in range(num_sub):\n",
    "            # Select every num_sub-th column of WV\n",
    "            WVone = WV[:, j::num_sub]\n",
    "#             print('shape of WVone: ', WVone.shape)\n",
    "            # Calculate the maximum score of each column of WVone multiplied by xbag\n",
    "            fs = np.max(xbag.T @ WVone, axis=0)\n",
    "#             print('shape of fs: ',fs.shape)\n",
    "            # Update the pres array with the maximum of the current pres and fs for the current bag and subset\n",
    "            pres[i,:] = np.maximum(pres[i,:], fs)\n",
    "            \n",
    "    # Sort the pres array in descending order for each bag, excluding the last column\n",
    "    ord = np.argsort(pres[:, :-1], axis=1)[:, ::-1]\n",
    "    \n",
    "    # Select the top firstk indices from the ord array\n",
    "    top_idx = ord[:, :firstk]\n",
    "    \n",
    "    # Create an array for storing the labels\n",
    "    labels = -np.ones((n, W.shape[1] // num_sub - 1))\n",
    "    \n",
    "    # Loop over each bag and set the label to 1 for the top index and any index where the pres score is greater than the last score plus 1\n",
    "    for i in range(n):\n",
    "        labels[i, top_idx[i, 0]] = 1\n",
    "        labels[i, pres[i, :-1] > (pres[i, -1] + 1)] = 1\n",
    "    \n",
    "    # Return the pres and labels arrays\n",
    "    return pres, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0be1d1d",
   "metadata": {},
   "source": [
    " \n",
    " CREATING A FUNCTION FOR USING MIMLFast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f932e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MIMLfast(train_data, train_targets, test_data):\n",
    "    # train_data: n*1 cells, one cell for a bag, each cell is a n_ins*d matrix\n",
    "    # train_targets: n*n_class, one row for a bag\n",
    "\n",
    "    ## parameters\n",
    "    D = 100  # dimension of the shared space\n",
    "    norm_up = 10  # norm of each vector (upper bound for each norm)\n",
    "    maxiter = 10  # number of iterations\n",
    "    step_size = 0.005  # step size of SGD\n",
    "    lambda_reg = 1e-5\n",
    "    num_sub = 5  # number of sub concepts\n",
    "    opts = {'norm': 1, 'average_size': 10, 'average_begin': 0}\n",
    "\n",
    "    ## initialization\n",
    "    #creating and adding a dummy variable:\n",
    "    train_targets = np.hstack((train_targets, np.ones((train_targets.shape[0], 1))*2))\n",
    "    n_class = train_targets.shape[1]\n",
    "\n",
    "    m = train_data[:][0][0].shape[0]\n",
    "    costs = 1.0 / np.arange(1, n_class+1).cumsum()\n",
    "\n",
    "# randomly generated V and W with mean 0 and standard deviation 1/sqrt(m)\n",
    "    V = np.random.normal(0, 1/np.sqrt(m), (D, m))  # D*m\n",
    "    W = np.random.normal(0, 1/np.sqrt(m), (D, n_class*num_sub))  # D*n_class\n",
    "    for k in range(m):\n",
    "        tmp1 = V[:, k]\n",
    "        V[:, k] = tmp1*norm_up/np.linalg.norm(tmp1)\n",
    "    for k in range(n_class*num_sub):\n",
    "        tmp1 = W[:, k]\n",
    "        W[:, k] = tmp1*norm_up/np.linalg.norm(tmp1)\n",
    "\n",
    "    AW = np.zeros((D, n_class*num_sub))\n",
    "    AV = np.zeros((D, m))\n",
    "    Anum = 0\n",
    "    trounds = 0   # no of rounds in SGD\n",
    "\n",
    "    ## train\n",
    "    for i in range(maxiter):\n",
    "        W, V, AW, AV, Anum, trounds = MIML_train(train_data, train_targets, W, V, costs, norm_up, step_size, num_sub, AW, AV, Anum, trounds, lambda_reg, opts)\n",
    "\n",
    "    ## test\n",
    "    test_outputs, test_labels = MIML_test(test_data, AW/Anum, AV/Anum, num_sub)\n",
    "    \n",
    "    return test_outputs, test_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740012b0",
   "metadata": {},
   "source": [
    "INITIATING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d78bd255",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outputs, test_labels = MIMLfast(train_data, train_targets, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf274c8b",
   "metadata": {},
   "source": [
    "RESULTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2526a41c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcf9bc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1. -1. -1. ... -1. -1. -1.]\n",
      " [ 1. -1. -1. ... -1. -1.  1.]\n",
      " [ 1. -1. -1. ... -1. -1.  1.]\n",
      " ...\n",
      " [ 1. -1. -1. ... -1. -1.  1.]\n",
      " [ 1. -1. -1. ... -1. -1.  1.]\n",
      " [ 1. -1. -1. ... -1. -1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "461551d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss: 0.27285714285714285\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "hamming_loss_val = hamming_loss(test_targets, test_labels)\n",
    "\n",
    "print(\"Hamming Loss:\", hamming_loss_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0ab907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8752ba78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2274ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
